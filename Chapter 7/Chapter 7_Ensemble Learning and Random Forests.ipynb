{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiM6HxITYW8J3IJQ5i3XXL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mardatillah/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/main/Chapter%207/Chapter%207_Ensemble%20Learning%20and%20Random%20Forests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7: Ensemble Learning and Random Forests\n",
        "---\n",
        "Pada chapter ini, dibahas tentang **Ensemble Learning**, yaitu teknik yang menggabungkan beberapa model machine learning untuk meningkatkan kinerja dan akurasi dibandingkan dengan model tunggal. **Random Forests** adalah salah satu metode ensemble yang paling populer.\n",
        "\n",
        "1. Voting Classifiers\n",
        "    *   **Voting Classifier** adalah metode ensemble di mana beberapa model (misalnya, beberapa classifier) digabungkan untuk menghasilkan prediksi. Tiga jenis utama adalah:\n",
        "        *   **Hard Voting:** Memilih kelas yang paling sering dipilih oleh model.\n",
        "        *   **Soft Voting:** Menghitung probabilitas untuk setiap kelas dan memilih kelas dengan probabilitas tertinggi.\n",
        "        *   **Weighted Voting:** Memberikan bobot lebih pada model yang lebih kuat (memiliki kinerja lebih baik).\n",
        "2. Bagging and Pasting\n",
        "    *   **Bagging (Bootstrap Aggregating)** adalah teknik yang mengurangi varians dengan melatih beberapa model independen pada data yang berbeda, yang diambil dengan pengambilan sampel acak dengan penggantian. Hasil prediksi dihitung berdasarkan rata-rata (untuk regresi) atau voting (untuk klasifikasi).\n",
        "        *   **Pasting** adalah versi Bagging yang menggunakan pengambilan sampel tanpa penggantian. Meskipun demikian, Bagging lebih sering digunakan karena lebih efektif dalam mengurangi varians.\n",
        "3. Random Forests\n",
        "    *   **Random Forests** adalah aplikasi dari **Bagging** yang menggunakan **decision trees** sebagai model dasar. Pada Random Forests, selain sampel acak dari data, fitur yang digunakan untuk setiap pembelahan juga dipilih secara acak, yang meningkatkan keberagaman antar pohon keputusan.\n",
        "        *   **Feature Bagging**: Dengan memilih subset acak dari fitur pada setiap pembelahan, Random Forests mengurangi korelasi antara pohon-pohon dan meningkatkan akurasi.\n",
        "        *   **Out-of-Bag (OOB) Evaluation**: Metode evaluasi internal yang digunakan untuk mengukur kinerja model tanpa menggunakan data uji. Setiap pohon di-fit menggunakan subset data, dan data yang tidak digunakan pada pembelahan (OOB) digunakan untuk evaluasi.\n",
        "4. Extra Trees\n",
        "    *   **Extra Trees (Extremely Randomized Trees)** adalah varian dari Random Forests yang lebih acak. Dalam Extra Trees, pemilihan fitur untuk pembelahan dan nilai threshold untuk pembelahan dilakukan secara acak, alih-alih memilih yang terbaik berdasarkan kriteria seperti **Gini impurity**.\n",
        "        *   Extra Trees dapat lebih cepat dan sering memberikan kinerja yang sangat baik, tetapi terkadang kurang akurat dibandingkan dengan Random Forests, terutama pada data yang sangat sensitif terhadap perubahan.\n",
        "5. Boosting\n",
        "    *   **Boosting** adalah teknik ensemble lain yang fokus pada model yang lemah (weak learners) dan meningkatkan kinerjanya secara bertahap. Model-model yang lebih baru diberi bobot lebih besar untuk mengoreksi kesalahan yang dibuat oleh model sebelumnya.\n",
        "        *   Contoh teknik boosting adalah **AdaBoost** dan **Gradient Boosting**. Boosting cenderung lebih sensitif terhadap data, dan dapat memberikan peningkatan yang signifikan dalam kinerja.\n",
        "6. Stacking\n",
        "    *   **Stacking** adalah metode ensemble yang melibatkan pelatihan model dasar (misalnya, pohon keputusan, SVM) dan kemudian melatih model final yang disebut **meta-learner** untuk menggabungkan prediksi dari model dasar tersebut.\n",
        "        *   Meta-learner ini dapat berupa model yang lebih kompleks (seperti regresi linear atau model lain), yang mencoba untuk mempelajari cara terbaik menggabungkan prediksi dari model-model dasar.\n",
        "7. Evaluating Ensemble Methods\n",
        "    *   Teknik ensemble umumnya memberikan kinerja yang lebih baik dibandingkan dengan model individu, terutama dalam mengurangi varians dan bias.\n",
        "        *   **Bias-Variance Trade-off:** Ensemble methods mengurangi varians tanpa meningkatkan bias yang signifikan, sehingga model lebih stabil dan cenderung lebih akurat.\n",
        "\n",
        "\n",
        "### Poin-poin Utama:\n",
        "*   **Ensemble Learning** menggabungkan beberapa model untuk meningkatkan kinerja.\n",
        "*   **Bagging** seperti pada **Random Forests** mengurangi varians dengan melatih beberapa model pada data yang berbeda.\n",
        "*   **Random Forests** menggunakan **decision trees** yang dilatih pada data dan fitur acak, serta **Out-of-Bag** evaluation untuk pengukuran kinerja.\n",
        "*   **Extra Trees** lebih acak daripada Random Forests dan dapat lebih cepat, tetapi terkadang kurang akurat.\n",
        "*   **Boosting** meningkatkan kinerja dengan memperbaiki kesalahan model sebelumnya, dengan contoh seperti **AdaBoost** dan **Gradient Boosting**.\n",
        "*   **Stacking** menggabungkan prediksi model dasar dengan menggunakan meta-learner untuk menghasilkan prediksi akhir.\n"
      ],
      "metadata": {
        "id": "YJ2sV76-ZD57"
      }
    }
  ]
}