{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOR2NeSkNSkOt6yv2RzEDt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mardatillah/Hands-on-Machine-Learning-with-Scikit-Learn-Keras-and-TensorFlow/blob/main/Chapter%2011/Chapter%2011_Training%20Deep%20Neural%20Networksip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 11: Training Deep Neural Networks\n",
        "---\n",
        "Chapter ini membahas teknik-teknik yang digunakan untuk melatih Deep Neural Networks (DNNs), yang terdiri dari banyak lapisan tersembunyi, serta tantangan dan solusi yang terkait dengan pelatihan model yang dalam.\n",
        "\n",
        "1. The Vanishing/Exploding Gradients Problems\n",
        "    *   Masalah Gradien Menghilang (Vanishing Gradients) dan Melebarnya Gradien (Exploding Gradients) adalah tantangan utama dalam pelatihan DNNs.\n",
        "        *   Vanishing Gradients terjadi ketika gradien menjadi sangat kecil saat melakukan backpropagation, yang menghambat pembelajaran.\n",
        "        *   Exploding Gradients terjadi ketika gradien menjadi sangat besar, menyebabkan pembaruan bobot yang tidak stabil.\n",
        "    *   Kedua masalah ini terutama terjadi pada jaringan yang sangat dalam, tetapi dapat diatasi dengan berbagai teknik yang dijelaskan dalam bab ini.\n",
        "\n",
        "2. Glorot and He Initialization\n",
        "    *   Inisialisasi Bobot yang baik sangat penting untuk melatih DNNs. Dua metode inisialisasi yang paling umum adalah:\n",
        "        *   Inisialisasi Glorot (Xavier): Cocok untuk fungsi aktivasi sigmoid atau tanh. Menginisialisasi bobot dengan distribusi yang memiliki varians yang seimbang.\n",
        "        *   Inisialisasi He: Lebih cocok untuk fungsi aktivasi ReLU, dan menggunakan distribusi yang lebih besar untuk mencegah masalah gradien menghilang.\n",
        "\n",
        "3. Nonsaturating Activation Functions\n",
        "    *   Fungsi aktivasi yang digunakan dalam DNNs harus memilih fungsi yang tidak menyebabkan saturasi gradien, seperti ReLU (Rectified Linear Unit). ReLU membantu mengurangi masalah gradien menghilang dan memungkinkan pelatihan lebih cepat.\n",
        "\n",
        "4. Batch Normalization\n",
        "    *   Batch Normalization adalah teknik yang digunakan untuk menstabilkan dan mempercepat pelatihan DNNs. Teknik ini menormalkan input ke setiap lapisan sehingga distribusi data tetap konstan selama pelatihan, yang membantu menghindari masalah gradien menghilang dan mempercepat konvergensi.\n",
        "\n",
        "5. Gradient Clipping\n",
        "    *   Gradient Clipping digunakan untuk menangani masalah exploding gradients. Dengan teknik ini, gradien yang terlalu besar dipotong (clipped) agar tidak melebihi nilai batas tertentu, sehingga mencegah pembaruan bobot yang tidak stabil.\n",
        "\n",
        "6. Reusing Pretrained Layers (Transfer Learning)\n",
        "    *   Transfer Learning melibatkan penggunaan lapisan-lapisan dari model yang telah dilatih sebelumnya pada dataset yang besar. Lapisan ini kemudian dapat disesuaikan dengan data dan tugas yang lebih kecil. Ini sangat berguna ketika data terbatas dan dapat mempercepat pelatihan model.\n",
        "        *   Pretrained Models: Model-model seperti VGG, ResNet, dan Inception yang telah dilatih pada dataset besar (misalnya, ImageNet) dapat digunakan kembali untuk berbagai tugas.\n",
        "\n",
        "7. Unsupervised Pretraining\n",
        "    *   Unsupervised Pretraining adalah metode untuk melatih lapisan pertama dari jaringan menggunakan data yang tidak memiliki label sebelum fine-tuning dengan data berlabel. Ini dapat membantu jaringan mempelajari fitur-fitur yang lebih baik untuk tugas berikutnya.\n",
        "\n",
        "8. Pretraining on an Auxiliary Task\n",
        "    *   Pretraining pada Tugas Pembantu melibatkan pelatihan model pada tugas tambahan yang terkait dengan tugas utama, sehingga model dapat belajar representasi yang lebih berguna sebelum melakukan pelatihan pada tugas yang lebih spesifik.\n",
        "\n",
        "9. Faster Optimizers\n",
        "    *   Optimizers yang lebih cepat seperti Momentum, Nesterov Accelerated Gradient, AdaGrad, RMSProp, dan Adam digunakan untuk mempercepat konvergensi pelatihan DNNs dan mengurangi masalah yang terkait dengan gradien.\n",
        "\n",
        "10. Momentum Optimization\n",
        "    *   Momentum adalah teknik optimisasi yang menambahkan momentum ke dalam pembaruan bobot. Ini membantu untuk mengatasi masalah yang terkait dengan laju pembelajaran yang terlalu tinggi atau terlalu rendah, dan mempercepat konvergensi dengan menambahkan komponen kecepatan pada gradien.\n",
        "\n",
        "11. Nesterov Accelerated Gradient\n",
        "    *   Nesterov Accelerated Gradient (NAG) adalah peningkatan dari momentum yang menghitung gradien pada posisi yang lebih baik di masa depan, bukan hanya posisi saat ini. Ini meningkatkan akurasi dan kecepatan konvergensi.\n",
        "\n",
        "12. AdaGrad, RMSProp, and Adam\n",
        "    *   AdaGrad, RMSProp, dan Adam adalah algoritma optimisasi yang dirancang untuk mengatasi kelemahan dari gradient descent tradisional, seperti pembelajaran yang lambat pada fitur dengan banyak varians.\n",
        "        *   Adam adalah salah satu optimizer yang paling populer karena menggabungkan keuntungan dari AdaGrad dan RMSProp, dengan mempertahankan kecepatan konvergensi yang lebih baik pada berbagai jenis masalah.\n",
        "\n",
        "13. Learning Rate Scheduling\n",
        "    *   Learning Rate Scheduling adalah teknik untuk menyesuaikan learning rate selama pelatihan. Hal ini memungkinkan model untuk memulai dengan learning rate yang lebih tinggi untuk eksplorasi, kemudian menurunkannya seiring waktu untuk konvergensi yang lebih halus.\n",
        "\n",
        "14. Avoiding Overfitting Through Regularization\n",
        "    *   Regularization adalah teknik untuk mencegah model terlalu cocok dengan data pelatihan, yang bisa mengarah pada overfitting. Beberapa teknik regularisasi yang umum digunakan termasuk:\n",
        "        *   L1 dan L2 Regularization\n",
        "        *   Dropout: Teknik yang menghilangkan unit secara acak selama pelatihan untuk mencegah model terlalu bergantung pada fitur tertentu.\n",
        "        *   Monte Carlo (MC) Dropout: Pendekatan berbasis probabilitas yang membantu model belajar representasi yang lebih baik dan lebih tahan terhadap overfitting.\n",
        "\n",
        "### Poin-poin Utama:\n",
        "*   Vanishing dan Exploding Gradients adalah masalah utama dalam pelatihan DNNs yang dapat diatasi dengan teknik seperti Glorot dan He Initialization, Batch Normalization, dan Gradient Clipping.\n",
        "*   Transfer Learning dan Unsupervised Pretraining dapat digunakan untuk melatih model lebih cepat dengan data terbatas.\n",
        "*   Optimizer yang lebih cepat seperti Adam dan Momentum membantu dalam mempercepat pelatihan.\n",
        "*   Regularization melalui teknik seperti Dropout dan L2 Regularization digunakan untuk mencegah overfitting."
      ],
      "metadata": {
        "id": "YJ2sV76-ZD57"
      }
    }
  ]
}